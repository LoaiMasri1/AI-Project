{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZn2pUVukPRZ"
      },
      "source": [
        "### **Import All Used Library**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sa0Ai1j4kIjl"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import winsound\n",
        "\n",
        "duration = 600  # milliseconds\n",
        "freq = 3500  # Hz\n",
        "\n",
        "# ref: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
        "# https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.scatter.html\n",
        "# http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
        "# https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
        "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html\n",
        "# https://docs.python.org/3/library/glob.html\n",
        "# https://docs.python.org/3/library/winsound.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfpfaR2bkdkv"
      },
      "source": [
        "### **Read xx_tracks.csv from data folder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LProux7SkIjn"
      },
      "outputs": [],
      "source": [
        "path = '../data/'\n",
        "file_name = glob.glob(path + \"/*_tracks.csv\")\n",
        "\n",
        "tracks = []\n",
        "\n",
        "for filename in file_name:\n",
        "    tracks.append(pd.read_csv(filename))\n",
        "    \n",
        "# ref: https://stackoverflow.com/questions/41262641/how-to-concatenate-multiple-dataframes-in-pandas\n",
        "# https://www.geeksforgeeks.org/how-to-read-all-csv-files-in-a-folder-in-pandas/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ohf9FZNwkqrh"
      },
      "source": [
        "### **define all utilities functions to calculate the mean (speed,accelaration,deccelartion)** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swlSkzOQkIjn"
      },
      "outputs": [],
      "source": [
        "# get the mean\n",
        "def mean_id(data, id):\n",
        "    return data[data['id'] == id].mean()\n",
        "\n",
        "# get the standard deviation\n",
        "def std_id(data, id):\n",
        "    return data[data['id'] == id].std()\n",
        "\n",
        "# get the length of specific track id\n",
        "def len_id(data, id):\n",
        "    return data[data['id'] == id].shape[0]\n",
        "\n",
        "# get the mean for deceleration\n",
        "def mean_deceleration(data, id):\n",
        "    return data[data['id'] == id]['xAcceleration'][data[data['id'] == id]['xAcceleration'] < 0].mean()\n",
        "\n",
        "# get the mean for acceleration\n",
        "def mean_acceleration(data, id):\n",
        "    return data[data['id'] == id]['xAcceleration'][data[data['id'] == id]['xAcceleration'] > 0].mean()\n",
        "\n",
        "# get the mean for acceleration\n",
        "def acceleration_id(data, id):\n",
        "    return data[data['id'] == id]['xAcceleration'].mean()\n",
        "\n",
        "# get the quantile for velocity\n",
        "def quantile_velocity(data, id, quantile):\n",
        "   return data[data['id'] == id]['xVelocity'].quantile(quantile)\n",
        "\n",
        "# get the quantile for acceleration\n",
        "def quantile_acceleration(data, id, quantile):\n",
        "    return data[data['id'] == id][data['xAcceleration'] > 0]['xAcceleration'].quantile(quantile)\n",
        "\n",
        "# get the quantile for deceleration\n",
        "def quantile_deceleration(data, id, quantile):\n",
        "    return data[data['id'] == id][data['xAcceleration'] < 0]['xAcceleration'].quantile(quantile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRt4WYJmlU_h"
      },
      "source": [
        "### Get a **unique list include id's of driver**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7O2vUGyOkIjo"
      },
      "outputs": [],
      "source": [
        "df = tracks[0] # get the first track for testing , my pc is slow to read all the tracks\n",
        "unique_id=df['id'].unique()\n",
        "\n",
        "# ref: https://thispointer.com/pandas-get-unique-values-in-single-or-multiple-columns-of-a-dataframe-in-python/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Jqw1PsXlgrA"
      },
      "source": [
        "### **Define functions to Find All velotity Mesures**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XTCJlGHkIjp"
      },
      "outputs": [],
      "source": [
        "# DV1 Loai\n",
        "def DV1(data, id):\n",
        "    mean =mean_id(data,id)['xVelocity']\n",
        "    length = len_id(data,id)\n",
        "    data['DV1'] = np.sqrt((data['xVelocity']-mean)**2)/length\n",
        "    \n",
        "# DV2 Loai\n",
        "def DV2(data, id):\n",
        "    mean =mean_id(data,id)['xAcceleration']\n",
        "    length = len_id(data,id)\n",
        "    data.loc[data['id']==id,'DV2'] = np.sqrt((data['xAcceleration'][data['id'] == id]-mean)**2/length)\n",
        "    \n",
        "    \n",
        "# DV6 Loai\n",
        "def DV6(data, id):\n",
        "    mean =mean_id(data,id)['xVelocity']\n",
        "    length = len_id(data,id)\n",
        "    data['DV6'] = np.abs(data['xVelocity']-mean)/length\n",
        "\n",
        "# DV3 Nabeel\n",
        "def DV3(data, id):\n",
        "    mean = mean_id(data,id)['xVelocity']\n",
        "    length = len_id(data,id)\n",
        "    data['DV3'] = (np.sqrt((data['xVelocity']-mean)**2/length)/mean) * 100\n",
        "\n",
        "# DV4 Nabeel\n",
        "def DV4(data, id):\n",
        "    mean = mean_acceleration(data,id)\n",
        "    length = len_id(data,id)\n",
        "    data.loc[data['id']==id,'DV4'] = (np.sqrt(\n",
        "        ((data['xAcceleration'][data['xAcceleration'] > 0][data['id'] == id]-mean)**2)/length))*100\n",
        "\n",
        "# DV5 Jehad\n",
        "def DV5(data, id):\n",
        "    mean = mean_deceleration(data,id)\n",
        "    length = len_id(data,id)\n",
        "    data.loc[data['id']==id,'DV5'] = (np.sqrt(\n",
        "        (data['xAcceleration'][data['xAcceleration'] < 0]-mean)**2/length)/mean) * 100\n",
        "\n",
        "# DV7 Jehad\n",
        "def DV7(data, id):\n",
        "    mean = mean_id(data,id)['xAcceleration']\n",
        "    length = len_id(data,id)\n",
        "    data['DV7'] = np.abs(data['xAcceleration']-mean)/length\n",
        "\n",
        "# DV8 Muthana\n",
        "def DV8(data, id):\n",
        "    Q1 = quantile_velocity(data,id,0.25)\n",
        "    Q3 = quantile_velocity(data,id,0.75)\n",
        "    data.loc[data['id']==id,'DV8'] = 100*((Q3-Q1)/(Q3+Q1))\n",
        "    \n",
        "# DV9 Muthana\n",
        "def DV9(data, id):\n",
        "    Q1 = quantile_acceleration(data,id,0.25)\n",
        "    Q3 = quantile_acceleration(data,id,0.75)\n",
        "    data.loc[data['id']==id,'DV9'] = 100*((Q3-Q1)/(Q3+Q1))\n",
        "    \n",
        "# DV10 Loai \n",
        "def DV10(data, id):\n",
        "    Q1 = quantile_deceleration(data,id,0.25)\n",
        "    Q3 = quantile_deceleration(data,id,0.75)\n",
        "    data.loc[data['id']==id,'DV10'] = 100*((Q3-Q1)/(Q3+Q1))\n",
        "    \n",
        "# DV11 Loai\n",
        "def DV11(data, id):\n",
        "    mean =mean_id(data,id)['xVelocity']\n",
        "    length = len_id(data,id)\n",
        "    number = (data[data['id'] == id]['xVelocity'] >= mean).shape[0]\n",
        "    data.loc[data['id']==id,'DV11'] = 100 * \\\n",
        "        (number+2*data['DV1'][data['id'] == id])/length\n",
        "        \n",
        "# DV12 Ahmad   \n",
        "def DV12(data, id):\n",
        "    mean = mean_acceleration(data,id)\n",
        "    length = len_id(data,id)\n",
        "    number = (data[data['id'] == id][data['xAcceleration'] > 0]\\\n",
        "        ['xAcceleration'] >= mean).shape[0]\n",
        "    data.loc[data['id']==id,'DV12'] = 100 * \\\n",
        "           (number+2*data['DV2'][data['id'] == id])/length\n",
        "           \n",
        "# DV13 Ahmad\n",
        "def DV13(data, id):\n",
        "    mean = mean_deceleration(data,id)\n",
        "    length = len_id(data,id)\n",
        "    number = (data[data['id'] == id][data['xAcceleration'] <= 0]['xAcceleration'] >= mean).shape[0]\n",
        "    data.loc[data['id']==id,'DV13'] = 100 * \\\n",
        "        (number+2*data['DV5'][data['id'] == id])/length\n",
        "\n",
        "# ref : https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html\n",
        "# ref : https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.agg.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xp4n9my0lNs6"
      },
      "outputs": [],
      "source": [
        "#DV10 Ousama wrong code\n",
        "import numpy as np\n",
        "for i in range(len(file_name)):\n",
        "    k = 0\n",
        "    file = pd.read_csv(file_name[i]+'_tracks.csv')\n",
        "    for j in range(1, len(file)):\n",
        "        id = file[(file['id'] == j)]\n",
        "        if len(id) == 0:\n",
        "            break\n",
        "        decel = -id['xAcceleration']\n",
        "        DV10 = ((np.quantile(decel, 0.75) - np.quantile(decel, 0.25)) /\n",
        "                (np.quantile(decel, 0.75) + np.quantile(decel, 0.25)))*100\n",
        "        print(DV10)\n",
        "\n",
        "#DV11 Ousama wrong code\n",
        "for i in range(len(file_name)):\n",
        "    k = 0\n",
        "    file = pd.read_csv(file_name[i]+'_tracks.csv')\n",
        "    for j in range(1, len(file)):\n",
        "        id = file[(file['id'] == j)]\n",
        "        if len(id) == 0:\n",
        "            break\n",
        "        length = len(id)\n",
        "        vMean = id['xVelocity'].mean()\n",
        "        count = 0\n",
        "        for f in range(length):\n",
        "            if abs(id['xVelocity'][k]) >= abs((vMean)+2):\n",
        "                count = count + 1\n",
        "            k = k+1\n",
        "        DV11 = 100*count/length\n",
        "        'print(DV11)'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKV6b6AKlAgd"
      },
      "source": [
        "### **find DV's For all for each id in data file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KigARrik_2D"
      },
      "outputs": [],
      "source": [
        "for i in range(len(unique_id)):\n",
        "    DV1(df, unique_id[i])\n",
        "    DV2(df, unique_id[i])\n",
        "    DV6(df, unique_id[i])\n",
        "    DV3(df, unique_id[i])\n",
        "    DV4(df, unique_id[i])\n",
        "    DV5(df, unique_id[i])\n",
        "    DV7(df, unique_id[i])\n",
        "    DV8(df, unique_id[i])\n",
        "    DV9(df, unique_id[i])\n",
        "    DV10(df, unique_id[i])\n",
        "    DV11(df, unique_id[i])\n",
        "    DV12(df, unique_id[i])\n",
        "    DV13(df, unique_id[i])\n",
        "    \n",
        "winsound.Beep(freq, duration) # play a beep sound when process is done\n",
        "\n",
        "# ref: https://stackoverflow.com/questions/17086263/how-to-play-a-sound-in-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vduo4nekk3Dq"
      },
      "source": [
        "### **export csv file after find DV's for each Driver**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrTEEXEkkIjq"
      },
      "outputs": [],
      "source": [
        "new_data = df[['DV1', 'DV2', 'DV3', 'DV4', 'DV5', 'DV6', 'DV7', 'DV8',\\\n",
        "               'DV9', 'DV10', 'DV11', 'DV12', 'DV13']].groupby(tracks[0]['id']).mean()\n",
        "\n",
        "new_data.fillna('0', inplace=True)\n",
        "new_data.to_csv('../new/new.csv') # generate a new csv file with all features for each track in the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi-9z0YLmay4"
      },
      "source": [
        "### **Read xx_tracksMeta.csv from data folder to get the class for each driver**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_drPnZTxkIjr"
      },
      "outputs": [],
      "source": [
        "path = '../data/'\n",
        "file_name = glob.glob(path + \"/*_tracksMeta.csv\")\n",
        "\n",
        "data = []\n",
        "\n",
        "for filename in file_name:\n",
        "    data.append(pd.read_csv(filename))\n",
        "\n",
        "# tracksMeta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLhkm_x3moju"
      },
      "source": [
        "### **Read xx_new.csv from new folder to add the class column into it**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxM7BKrwkIjr"
      },
      "outputs": [],
      "source": [
        "path = '../new/'\n",
        "file_name = glob.glob(path + \"/*_new.csv\")\n",
        "\n",
        "new = []\n",
        "\n",
        "for filename in file_name:\n",
        "    new.append(pd.read_csv(filename))\n",
        "\n",
        "# new data only with DV "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rz8EXzZjm2-8"
      },
      "source": [
        "### **add the class column into new dataframe and export it into data_new** \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpDV48stkIjs"
      },
      "outputs": [],
      "source": [
        "class_names = []\n",
        "for i in range(len(data)):\n",
        "    class_names.append(pd.Series(data[i]['class'].values, index=data[i]['id']))\n",
        "\n",
        "for i in range(len(new)):\n",
        "    new[i]['class'] = new[i]['id'].map(class_names[0])\n",
        "\n",
        "for i in range(len(new)):\n",
        "    new[i].to_csv('../data_new/'+'new_' + str(i+1) + '.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5rjj8IcnOzz"
      },
      "source": [
        "### **Read all files with .csv extention in new_data folder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YDZ8TXCkIjs"
      },
      "outputs": [],
      "source": [
        "path = '../data_new/'\n",
        "file_name = glob.glob(path + \"/*.csv\")\n",
        "\n",
        "data = []\n",
        "\n",
        "for filename in file_name:\n",
        "    data.append(pd.read_csv(filename))\n",
        "    \n",
        "# new data with DV and class name for each driver id\n",
        "\n",
        "data = pd.concat(data) # concat all data into one dataframe \n",
        "\n",
        "# ref: https://pandas.pydata.org/docs/reference/api/pandas.concat.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FONfA2znjDl"
      },
      "source": [
        "### **to check if there is any classes not truck and car**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REazJEwfkIjt"
      },
      "outputs": [],
      "source": [
        "class_name=[]\n",
        "classes = ['Car', 'Truck']\n",
        "\n",
        "class_name.append(data['class'].unique())\n",
        "\n",
        "count=0\n",
        "for i in range(len(class_name)):\n",
        "    if classes[0] not in class_name[i] and classes[1] not in class_name[i]:\n",
        "        count+=1\n",
        "        \n",
        "if count==0:\n",
        "    print('All class name is car and truck')\n",
        "else:\n",
        "    print('There is class name not car and not truck')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQysha0inqjB"
      },
      "source": [
        "### **Elbow Methode to get the optimal number of cluster's**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDSBTe4HkIju"
      },
      "outputs": [],
      "source": [
        "cluster=data.drop(['id','class'], axis=1)\n",
        "x=cluster.iloc[:,:-1].values\n",
        "\n",
        "# sum of squared distance between each point and the centroid in a cluster\n",
        "wcss=[]\n",
        "for i in range(1,11):\n",
        "    kmeans=KMeans(n_clusters=i,init='k-means++',max_iter=300,n_init=10,random_state=0)\n",
        "    kmeans.fit(x)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "plt.plot(range(1,11),wcss)\n",
        "plt.title('The Elbow Method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('WCSS')\n",
        "plt.show()\n",
        "\n",
        "# elbow method\n",
        "\n",
        "# ref : https://www.analyticsvidhya.com/blog/2021/05/k-mean-getting-the-optimal-number-of-clusters/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll1GSDRFnvqD"
      },
      "source": [
        "### **drop the column with 25% zero's to prepare data to clustering** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFTldxfNkIju"
      },
      "outputs": [],
      "source": [
        "percentage_zeros = 0.25  # 25% of the data will be zeros\n",
        "\n",
        "cluster = data.drop(['id', 'class'], axis=1)\n",
        "\n",
        "\n",
        "drop_cols = cluster.columns[(cluster == 0).sum() >\n",
        "                            percentage_zeros*cluster.shape[1]]\n",
        "cluster.drop(drop_cols, axis=1, inplace=True)\n",
        "\n",
        "# ref: https://thispointer.com/pandas-drop-dataframe-columns-based-on-nan-percentage/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AMoS5oln8xw"
      },
      "source": [
        "### **clustering the data with K-mean Algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlTab105kIjv"
      },
      "outputs": [],
      "source": [
        "n_clusters = 2\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaler_feature=scaler.fit_transform(cluster)\n",
        "k_mean = KMeans(n_clusters=n_clusters, init='k-means++',max_iter=30, n_init=10, random_state=24)\n",
        "k_mean.fit(scaler_feature)\n",
        "\n",
        "y_predict = k_mean.predict(scaler_feature)\n",
        "\n",
        "plt.scatter(scaler_feature[y_predict == 0, 0],scaler_feature[y_predict == 0, 1], s=20, c='r', label='Cluster 1')\n",
        "plt.scatter(scaler_feature[y_predict == 1, 0],scaler_feature[y_predict == 1, 1], s=20, c='g', label='Cluster 2')\n",
        "plt.scatter(k_mean.cluster_centers_[:, 0], k_mean.cluster_centers_[:, 1], s=300, c='black', marker='*')\n",
        "plt.legend()\n",
        "plt.title('K-Means')\n",
        "\n",
        "# ref: https://datatofish.com/k-means-clustering-python/\n",
        "# https://blog.dominodatalab.com/getting-started-with-k-means-clustering-in-python\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vd1Lm04joP3S"
      },
      "source": [
        "### **generate a csv file with the describe of each feature**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KKzMsJ1kIjv"
      },
      "outputs": [],
      "source": [
        "desc = data.drop(['id', 'class'], axis=1)\n",
        "desc.describe().to_csv('describe.csv') # generate a csv file with the describe of each feature\n",
        "\n",
        "# ref: https://www.machinelearningplus.com/pandas/pandas-describe/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U99PA-rpoSL2"
      },
      "source": [
        "### **Scaled Cluster Centers Using only the data with DV**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLEK3QFZkIjv"
      },
      "outputs": [],
      "source": [
        "# Scaled Cluster Centers Using all the data\n",
        "\n",
        "# Scaled Cluster Centers Using only the data with DV\n",
        "\n",
        "data_dv = data.drop(['id', 'class'], axis=1)\n",
        "drop_cols = data_dv.columns[(data_dv == 0).sum() >\\\n",
        "                            percentage_zeros*data_dv.shape[1]]\n",
        "data_dv.drop(drop_cols, axis=1, inplace=True)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaler_feature = scaler.fit_transform(data_dv)\n",
        "scale_labels_dv = k_mean.fit_predict(scaler_feature)\n",
        "scale_centers_dv = k_mean.cluster_centers_\n",
        "\n",
        "scaled_output_dv = pd.DataFrame(\n",
        "    scale_centers_dv, columns=data_dv.columns, index=['Cluster 1', 'Cluster 2'])\n",
        "\n",
        "scaled_output_dv.to_csv('../clusters/scaled_output_dv.csv') # Scaled Cluster Centers\n",
        "\n",
        "# ref : https://scikit-learn.org/stable/modules/clustering.html#clustering\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "dZn2pUVukPRZ",
        "QfpfaR2bkdkv",
        "Ohf9FZNwkqrh",
        "oRt4WYJmlU_h",
        "1Jqw1PsXlgrA",
        "aKV6b6AKlAgd",
        "Vduo4nekk3Dq",
        "zi-9z0YLmay4",
        "BLhkm_x3moju",
        "Rz8EXzZjm2-8",
        "T5rjj8IcnOzz",
        "4FONfA2znjDl",
        "vQysha0inqjB",
        "Ll1GSDRFnvqD",
        "2AMoS5oln8xw",
        "vd1Lm04joP3S",
        "U99PA-rpoSL2"
      ],
      "name": "main.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "4f5f32c48f19cb6c7211461d1b54fa1dbf7bcde426790c18c187dd08ba2af639"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 64-bit (windows store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
